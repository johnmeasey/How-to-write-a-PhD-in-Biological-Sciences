# Are researchers writing more, and is more better? {#publishmore}

The idiom ‘publish or perish’ suggests that researchers will increase their output in order to obtain positions and promotions. And if a researcher’s productivity is measured by their publication output, shouldn't we all be writing more papers? Certainly, it appears that more papers are being published (see Figure \@ref(fig:authors-writing-more)). An estimate for the total number of scholarly articles in existence was 50 million in 2009, with more than half of these published in the years from 1984 [@jinha2010article].

Similarly, if we are all be writing more, then wouldn't some people start publishing two (or more) papers, when one would be adequate? This idea of ‘[salami slicing](#salami)’ to inflate outputs would be an understandable strategy if researchers were all trying to increase their output. Alternatively, the names of authors might be added to papers in which they did not make significant input via ghost [authorship or hyperauthorship](#authors) [see @cronin2001hyperauthorship for an interesting historical perspective, and [Part I](#authors)]. 


(ref:authors-writing-more) **The growth in the number of papers published in Life Sciences over time.** The number of papers published (blue line) compared to a standard growth rate (black line). The data come from www.scopus.com.

```{r authors-writing-more, echo=FALSE, fig.cap="(ref:authors-writing-more)"}
if(knitr::is_html_output(excludes="markdown")) knitr::include_url("https://johnmeasey.github.io/authors-writing-more/index.html", height="450px") else knitr::include_graphics("figures/authors-writing-more/authors-writing-more.png")
```


A study by Fanelli and Larivière [-@fanelli2016researchers] has a new take on the above questions, by asking whether researchers are actually writing more papers now than they did 100 years ago. They used the Web of Science to look for unique authors (more than half a million of them) and determine whether the first year of publication and the total number of publications resulted in an increasing trend. Fanelli and Larivière’s [-@fanelli2016researchers] trend line for biology is very stable at around 5.5 publications whether you started publishing in 1900 or 2000 (note that earth science and chemistry do both increase dramatically).

But it is possible that these figures could be explained by the fact that the culture in publishing in biological sciences has changed a great deal since then. One hundred years ago, it was very unlikely that any postgraduate students would publish articles in peer reviewed journals. Moreover, it was also acceptable for advisers to take the thesis work of their students and write it up in monographs. This has certainly changed with the ranks of authors now being swelled considerably so that many more authors are likely to be included on only a single publication in which they participated [see @measey2011past]. I interpret this as the biological sciences becoming more democratic, with more of the people that contribute to the work receiving the [credit](#authors).  

### At what rate is the literature increasing?
Using several databases (Web of Science, Scopus, Dimensions and Microsoft Academic) back to the beginning of their collections at the start of scientific journals in the mid 1600s, Bornmann et al. [-@bornmann2020growth] caluclated the inflation rate of scientific literature to run at 4.02%, such that the literature will double in 16.8 years [@bornmann2020growth]. This means that there is literally twice as much published in 2020 as there was in 2003. 

Although the early period of scientific publishing was notably slower than today, it is since the mid-1940s (following the end of 'World War II') that science has seen an exponential growth in productivity, with annual growth of 5.1%, and a doubling time of 13.8 years [@bornmann2020growth].

### If more is being published, will Impact Factors increase?
Yes, there is inflation of impact factors. If the numbers of citations per paper remains constant, then the [Impact Factor](#impactfactor) of journals should increase annually at 5%. My impression [see @measey2011past] is that citations are increasing in papers as the literature increases, which suggests that Impact Factor will grow at a faster rate . 


## Are some authors unfeasibly prolific?

How many articles could you publish in a year before you could be described as 'unfeasibly prolific'? This question was posed in a study that examined prolific authors in four fields of medicine [@wager2015too]. This publication piqued my interest as it turns out that the authors decided that researchers with more than 25 publications in a year were unfeasibly prolific, as this would be the equivalent of “>1 publication per 10 working days”. Their angle was to suggest that publication fraud was likely, and that funders should be more circumspect when accepting researchers productivity as a metric. Looking back through the peer review of this article (which is a great aspect of many PeerJ articles), I’m astounded that only one reviewer questioned the premise that it’s infeasible to author that number of papers in a year.

I have published >25 papers and book chapters in a year, and I know other people who do this regularly. To me, there is no question that (a) it is possible and (b) that they really are the authors - with no question of fraud. Firstly, the idea that prolific authors constrain their activity to “working days” is naïve. Most will be working throughout a normal weekend, and working in the early morning and late evening, especially in China [see @barnett2019working]. A hallmark of a prolific author would be emails early in the morning and/or late at night. This gives you an indication of their working hours, and how they are struggling to keep up with correspondence on top of writing papers. 

Authorship of a publication is often the result of several years of work, and it can be at many different levels of investment (see [Part 1](#authors)). Thus, from my perspective, when I look at authoring a lot of publications it reflects the activity of the initial concept for the work, raising of money, conducting the field work or experiment, analysing the data and then writing it up (with the subsequent submission and peer review time - see [Part I](#authors)). Often, the work conducted by students who lead the publications, many years later, are the culmination of many years of investment of both time and money. And in the Biological Sciences, good study systems keep giving. 


## Salami-slicing {#salami}
'Salami-slicing' is different things to different people. While many people refer to salami-slicing as the multiplication of papers into as many papers as possible, or slicing research into 'least publishable units', others are referring to publishing the same paper more than once in different journals. In the latter case, this should be considered self-plagiarism at best and fraud at worst. If you find examples of dual publications, these should result in [retractions](#retract). Instances don't need to be exact copies. I edited a submission where one of the referees alerted me to the fact that the same data with a very similar question had already been published two years previously. In this instance, I passed the submission to the ethical panel of the journal who rejected it, and flagged the authors for scrutiny in the case of future submissions. Note that instances where conference abstracts are printed in a journal does not prevent you from publishing a full paper of this work. I would maintain, however, that you should rewrite the abstract to prevent [self-plagiarism](http://howtowriteaphd.com/plagiarise.html) [@measey2021how]. 

During the production of any dataset, you are likely to find that you are able to answer more questions that you originally set out to ask when you first proposed the research (i.e. preregistration; see [Part I](#preprints2)). The question you will be left with is: whether you should be adding these _post hoc_ questions (questions that arise after the study) to the manuscripts that you planned to write when you proposed the research, or whether these should be published as separate publications - clearly identified as _post hoc_ questions? The realities of publishing in scientific journals means that in many instances you will be restricted by the number of words a journal will accept. This will mean that for certain outputs it will not be possible for you to ask additional _post hoc_ questions, or potentially all the questions you wanted to report in the [preregistered plan](#commitment).

There is nothing wrong with writing papers based purely on findings that you came across during the study: _post hoc_ questions. There is a clear role in scientific publishing of natural history observations. But that any publication (or part of a publication) that results must indicate that it results from a _post hoc_ study. To my way of thinking, it would be more useful if journals had separate sections for such studies, with other publications only stemming from from those that can show a [preregistered plan](#commitment). This would clearly improve [transparency](#transparency2) in publishing, and avoid accusations of [p-hacking or HARKing](#transparency).

At what point does the separation of research questions into different papers become 'salami slicing'? There is no simple answer to this question, and editors are likely to disagree [@tolsgaard2019salami-slicing]. However, there are ways in which you as an author can make sure that your work is [transparent](#transparency2), and therefore that you are not accused of 'salami slicing'. First is the [preregistration](#commitment) of your research plan. Second is to [preprint](#preprints2) any unpublished papers that are referenced in your submission. There are also guidelines from COPE on the: '[Systematic manipulation of the publication process](https://publicationethics.org/resources/flowcharts/systematic-manipulation-publication-process)' [@cope2018systematic]. And the last is to be transparent when you publish _post hoc_ research.

In manuscripts where another very similar study is cited by the authors, but not available to reviewers or editors, there should be a 'salami slicing' red flag. Obviously, when you produce a number of outputs from a research project, they are likely to be linked and therefore [cited by each other](#self). However, when these are not available to reviewers and editors (as [preprints](#preprints2) or as [preregistration](#commitment) of the questions), authors should expect to be asked for these manuscripts to demonstrate that they are not salami-sclicing. Perhaps worse, however, is when authors deliberately hide any citation to another very similar work. In the end, we have to rely on the integrity of the researchers not to be unethical or dishonest. 

## Is writing a lot of papers a good strategy? 
This is a question of long standing, and one that you may find yourself asking at some point early on in your career. I'd suggest that the answer will be more about the sort of person that you are, or the lab culture you experience, over any strategy that you might consciously decide. If you tend toward perfectionism, this will likely result in fewer papers that (I hope) you'd consider to be of high quality. If on the other hand your desire were to finish projects and move on, you'd be more likely to tend toward more papers. It is clear that the current climate leads towards the latter strategy, with increasing numbers of early career researchers bewildered at the idea of increasing their publication metrics [@helmer2020what]. But what should you do?

Given that the 'best' personality type lies somewhere in the middle, you can decide for yourself whether you identify with one side more than the other. But which is the better strategy? Vincent Larivière and Rodrigo Costas [-@lariviere2016how] tried to answer this question by considering how many papers unique authors wrote and seeing how this relates to their share of authoring a paper in the top 1% of cited papers. Their result showed clearly that for researchers in the life sciences, writing a lot of papers was a good strategy if you started back in the 1980s. However, for those starting after 2009, the trend was reversed with those authors writing more papers less likely to have a 'smash hit' paper (in the top 1% of cited papers). Maybe the time scale was too short to know. After all, if you started publishing in 2009 and had >20 papers by 2013 then you have been incredibly (but not unfeasibly) prolific. Other studies continue to show that in the life sciences, writing more papers still provides returns towards having papers highly cited: the more papers you author, the higher the chance of having a highly cited paper [@sandstrom2016quantity]. 

One aspect not considered Larivière and Costas [-@lariviere2016how] is that becoming known as a researcher who finishes work (resulting in a publication) is likely to make you more attractive to collaborators. Thus, publishing work is likely to get you invited to participate in more work. Obviously, quality plays a part in invitations to collaborative work too. Thus pulling the argument back to the centre ground. 

There are other scenarios in which you might be encouraged to write more. In Denmark, for example, research funding is apportioned to universities based on the number of outputs their researchers generated in a point system, where higher ranked journals get more points. This resulted in researchers in the life sciences changing their publication strategy with a notable increase in publications in the highest points bracket following this change [@deutz2021quantitative]. 

You may find yourself becoming preoccupied about which is the best strategy for you, not because you want to, but because your institution is relying on you to pull your weight in their assessment exercise. University rankings are now very important, and big universities like to be ranked highly for research, which depends (in part) on the quantity and quality of their output. 

### Natural selection of bad science {#badscience}
In [-@smaldino2016natural], Smaldino and McElreath proposed that ever increasing numbers of publications not only leads to bad science, but is currently selected for in an academic environment where publishing is considered as a currency. They argued that the most productive laboratories will be rewarded with more grant funding, larger numbers of students, and that these students will learn about the methods and benefits of prolific publication. When these ‘offspring’ of the prolific lab look for jobs, they are more likely to be successful as they have more publications themselves. An academic environment that rewards increasing numbers of publications eventually selects towards methodologies that produce the greatest number of publishable results. To show that this leads to a culture of ‘bad science’, Smaldino and McElreath [-@smaldino2016natural] conducted an analysis in trends over time of statistical power in behavioural science publications. Over time, better science should be shown by researchers increasing their statistical power as this will provide studies with lower error rates. However, increasing the statistical power of experiments takes more time and resources, resulting in fewer publications. Their results, from review papers in social and behavioural sciences, suggested that between 1960 and 2011 there had been no trend toward increasing statistical power. Biological systems, whether they be academics in a department or grass growing in experimental pots, will respond to the rewards generated in that system. When grant funding bodies and academic institutions reward publishing as a behaviour, it is inevitable that the behaviour of researchers inside that system will respond by increasing their publication output. Moreover, if those institutions maintain increasing numbers of researchers in temporary positions, those individuals are further incentivised to become more productive to justify their continued contracts, or the possibility of obtaining a (more permanent) position elsewhere. Eventually, this negative feedback, or gameification of publishing metrics, produces a dystopian and dysfunctional academic reality [@helmer2020what].

An example of this kind of confirmation bias driven publishing effect toward bad science can be found in the literature of fluctuating asymmetry, and in particular those studies on human faces [@dongen2011associations]. Back in the 1990s, there was a flurry of high profile articles purporting preference for symmetry (and against asymmetry) in human faces. The studies were (relatively) cheap and fast to conduct as the researchers had access to hundreds of students right on their doorsteps. The studies not only hit the top journals, but were very popular in the mainstream media as scientists were apparently able to predict which faces were the most attractive. Stefan van Dongen [-@dongen2011associations] hypothesised that if publication bias was leading to bad science in studies of fluctuating asymmetry in human faces, there would be a negative association between effect size and sample size when fluctuating asymmetry in human faces. Effect sizes are expected to be smaller in larger in studies with high sample sizes as these come with less accurate measurements [see @jennions2002publication], but we should not expect to see this relationship change depending on the stated focus of the study. However, van Dongen found that this relationship was diminished in studies when fluctuating asymmetry in human faces was not the main aim of the study, suggesting that there was important publication bias.  

Where others have looked, publication bias has been found and is particularly associated with a decreasing effect size that correlates with journal [Impact Factor](#impactfactor): i.e. once the large effect is published in a big journal, the natural selection of bad science results in publication bias, and diminishing effect sizes that ripple through lower impact factor journals [@munafo2007association; @brembs2013deep; @smaldino2016natural], while negative results disappear almost entirely [@fanelli2012negative]. One can presume that negative results exist, but their authors either do not bother to write up the results, or that journals won't publish them. 

The direct result of a system driven by [Impact Factor](#impactfactor) and author publication metrics is that we will have a generation of scientists at the top institutions that are trained not to conduct the best science, but to generate publications that can be sold to the best journals. We should be deeply suspicious of any claim of linkage between top journals and quality [@brembs2013deep]. Indeed, what we see increasingly is that the potential rewards of publishing in top [Impact Factor](#impactfactor) journals leads not only to bad science, but increasingly to [deliberate fraud](#pruittdata). Continuing along this path threatens to undermine the entire [scientific project](http://howtowriteaphd.com/lifescientific.html), and places science and scientists as just another stakeholder in a system ruled by economic markets, and their promotion of the fashion of the day [@brembs2013deep; @casadevall2012reforming].


